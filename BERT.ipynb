{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from random import *\n",
    "# import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #to reproduce results\n",
    "# SEED=1234\n",
    "\n",
    "# random.seed(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch():\n",
    "  batch=[]\n",
    "  positive = negative = 0\n",
    "  #when count of positive and negative sentences = batch_size, dont proceed\n",
    "  while positive != batch_size/2 or negative != batch_size/2 :\n",
    "    #selects random tokens_a_index and tokens_b_index from [0,len(sentences))\n",
    "    tokens_a_index,tokens_b_index = randrange(len(sentences)),randrange(len(sentences))\n",
    "    #token_list is list of sentence tokens\n",
    "    tokens_a, tokens_b =token_list[tokens_a_index],token_list[tokens_b_index]\n",
    "    input_ids = [word_dict['[CLS]']] +tokens_a +[word_dict['[SEP]']] +tokens_b +[word_dict['[SEP]']]\n",
    "    segment_ids= [0] * (1+len(tokens_a)+1) + [1] * (len(tokens_b) +1)\n",
    "\n",
    "    #n_pred - max mask we can apply\n",
    "    n_pred = min(max_pred, max(1, int(round(len(input_ids)*0.15))))\n",
    "\n",
    "    sent12 = [i for i,token in enumerate(input_ids) if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "    shuffle(sent12)\n",
    "    masked_tokens,masked_pos=[],[]\n",
    "    for pos in sent12[:n_pred]:\n",
    "      masked_pos.append(pos)\n",
    "      masked_tokens.append(input_ids[pos])\n",
    "      if random()<0.8:\n",
    "        input_ids[pos] = word_dict['[MASK]']\n",
    "      elif random()<0.9:\n",
    "        index=randrange(vocab_size)\n",
    "        input_ids[pos] = index  \n",
    "        \"\"\"change\"\"\"\n",
    "      else:\n",
    "        pass #mask token will remains unchanged\n",
    "      \n",
    "    #in paper it is mentioned that model is trained for max seq len of 128 for 90% and 512 for 10%\n",
    "    n_pad=max_len - len(input_ids)\n",
    "    input_ids.extend([0]*n_pad)\n",
    "    segment_ids.extend([0]*n_pad)\n",
    "\n",
    "    #this step is not clear\n",
    "    if max_pred > n_pred:\n",
    "      n_pad = max_pred - n_pred\n",
    "      masked_tokens.extend([0] * n_pad)\n",
    "      masked_pos.extend([0] * n_pad)\n",
    "\n",
    "    if tokens_a_index+1 == tokens_b_index and positive < batch_size/2:\n",
    "      batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])\n",
    "      positive+=1\n",
    "\n",
    "    elif tokens_a_index+1 != tokens_b_index and negative < batch_size/2:\n",
    "      batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])\n",
    "      negative+=1\n",
    "    \n",
    "  return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q,seq_k):\n",
    "  #here len_q = len_k = len_sent \n",
    "  batch_size, len_q= seq_q.size()\n",
    "  batch_size, len_k= seq_k.size()\n",
    "  pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  #batch_size x 1 x len_k(=len_q)\n",
    "  # mask=0=true and othertoken=1=false\n",
    "  return pad_attn_mask.expand(batch_size,len_q,len_k) \n",
    "\n",
    "def gelu(x):\n",
    "    return x*0.5*(1.0+torch.erf(x/math.sqrt(2.0)))\n",
    "    #Computes the error function of each element. The error function:\n",
    "    #erf(x) = (2/sqrt(pi))int(e^(-t^2)dt)\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Embedding, self).__init__()\n",
    "    self.tok_embed=nn.Embedding(vocab_size,h_dim)\n",
    "    self.pos_embed=nn.Embedding(max_len,h_dim)\n",
    "    self.seg_embed=nn.Embedding(n_segments,h_dim)\n",
    "    self.norm=nn.LayerNorm(h_dim)\n",
    "\n",
    "  def forward(self,x,seg):\n",
    "    seq_len= x.size(1)\n",
    "    pos = torch.arange(seq_len, dtype=torch.long)\n",
    "    pos = pos.unsqueeze(0).expand_as(x)\n",
    "    embedding = self.tok_embed(x)+self.pos_embed(pos)+self.seg_embed(seg)\n",
    "    #Layer Normalization of sum(embedding) \n",
    "    return self.norm(embedding)\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(ScaledDotProductAttention,self).__init__()\n",
    "\n",
    "  def forward(self, Q, K, V, attn_mask):\n",
    "    #Q=[batch_size,n_heads,q_len,d_k] K=[batch_size,n_heads,k_len,d_k]\n",
    "    scores=torch.matmul(Q,K.permute(0,1,3,2))/np.sqrt(d_k)\n",
    "    scores.masked_fill_(attn_mask, -1e9) #Fills elements of scores with -1e9 where mask is one.\n",
    "    attn=torch.softmax(scores,dim=-1)\n",
    "    context=torch.matmul(attn,V)\n",
    "    return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    #d_q=d_k\n",
    "    self.W_Q = nn.Linear(h_dim, d_k * n_heads)\n",
    "    self.W_K = nn.Linear(h_dim,d_k * n_heads)\n",
    "    self.W_V = nn.Linear(h_dim, d_v *n_heads)\n",
    "    self.fc_o = nn.Linear(n_heads * d_v, h_dim)\n",
    "    self.norm=nn.LayerNorm(h_dim)\n",
    "\n",
    "  def forward(self,Q,K,V, attn_mask):\n",
    "    batch_size=Q.shape[0]\n",
    "\n",
    "    q_s=self.W_Q(Q).view(batch_size, -1, n_heads, d_k).permute(0,2,1,3) #q_s=[batch_size, n_heads,q_len,d_k]\n",
    "    k_s=self.W_K(K).view(batch_size, -1, n_heads, d_k).permute(0,2,1,3) #k_s=[batch_size,n_heads,k_len,d_k]\n",
    "    v_s=self.W_V(V).view(batch_size, -1, n_heads, d_v).permute(0,2,1,3) #v_s=[batch_size,n_heads,v_len,d_v]\n",
    "\n",
    "    attn_mask = attn_mask.unsqueeze(1).repeat(1,n_heads,1,1)\n",
    "    #so dim 1 gets repeated by n_heads and other dims (0,2,3) are not repeated as repeat factor=1\n",
    "    # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "    ScaledDotProductAttentionInstant = ScaledDotProductAttention()\n",
    "    context,attn = ScaledDotProductAttentionInstant(q_s,k_s,v_s,attn_mask)\n",
    "    #context=[batch_size,n_heads,q_len,d_v]\n",
    "    context = context.permute(0,2,1,3).contiguous()\n",
    "    #context=[batch_size,q_len,n_heads,d_v]\n",
    "    context = context.view(batch_size, -1 , n_heads*d_v)\n",
    "    #context=[batch_size,q_len,n_heads*d_v]\n",
    "    output = self.fc_o(context)\n",
    "    #output=[batch_size,q_len,h_dim]\n",
    "    return self.norm(output+Q), attn\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "  def __init__(self):\n",
    "      super(PoswiseFeedForwardNet,self).__init__()\n",
    "      self.fc1 = nn.Linear(h_dim,d_ff)\n",
    "      self.fc2 = nn.Linear(d_ff, h_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "      return self.fc2(gelu(self.fc1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "  def __init__(self):\n",
    "      super(EncoderLayer, self).__init__()\n",
    "      self.enc_self_attn = MultiHeadAttention()\n",
    "      self.pos_ffn = PoswiseFeedForwardNet()\n",
    "\n",
    "  def forward(self,enc_inputs,enc_self_attn_mask):\n",
    "      enc_outputs , attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)\n",
    "      enc_outputs = self.pos_ffn(enc_outputs)\n",
    "      return enc_outputs, attn\n",
    "\n",
    "class BERT(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BERT,self).__init__()\n",
    "    self.embedding = Embedding()\n",
    "    embed_weight=self.embedding.tok_embed.weight\n",
    "    self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "    self.fc = nn.Linear(h_dim,h_dim)\n",
    "    self.linear = nn.Linear(h_dim,h_dim)\n",
    "    self.activ1=nn.Tanh()\n",
    "    self.norm=nn.LayerNorm(h_dim)\n",
    "    self.classifier=nn.Linear(h_dim,2)\n",
    "    n_vocab,n_dim = embed_weight.size()\n",
    "    self.decoder=nn.Linear(n_dim,n_vocab,bias=False)\n",
    "    self.decoder.weight = embed_weight\n",
    "    self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "  def forward(self,input_ids,segment_ids,masked_pos):\n",
    "    #shape(input_ids) = shape(segment_ids) = shape(masked_pos) =(batchsize, sent_length)\n",
    "    output = Embedding()(input_ids, segment_ids)\n",
    "    enc_self_attn_mask = get_attn_pad_mask(input_ids,input_ids)\n",
    "    for layer in self.layers:\n",
    "      output, enc_self_attn =layer(output, enc_self_attn_mask) #output=[batch_size,sent_len,h_dim]\n",
    "      \n",
    "    \n",
    "    #extracting cls token\n",
    "    h_pooled = self.activ1(self.fc(output[:,0])) #output[:,0]=[batch_size,h_dim]\n",
    "    \"\"\"2\"\"\"\n",
    "    logits_clsf = self.classifier(h_pooled)\n",
    "\n",
    "    #basically None adds newaxis at that position i.e. 3rd dim is introduced here.\n",
    "    \"\"\"print('output',masked_pos )\"\"\"\n",
    "    #masked_pos =[batch_size,mask_len] mask_len<5 due to considered conditions\n",
    "    masked_pos = masked_pos[:,:,None].expand(-1,-1,output.size(-1)) #masked_pos =[batch_size,mask_len,h_dim] this is done to select embeddings of mask tokens\n",
    "    \"\"\"print('output_size',masked_pos)\"\"\"\n",
    "    \n",
    "    # get embeddings of only mask tokens from final output of transformer\n",
    "    h_masked = torch.gather(output,1,masked_pos)\n",
    "    \n",
    "    h_masked = self.norm(gelu(self.linear(h_masked)))\n",
    "    logits_lm = self.decoder(h_masked) + self.decoder_bias\n",
    "    #logits_lm=[batch_size,mask_len,vocab_size\n",
    "    return logits_lm, logits_clsf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  #BERT parameters\n",
    "  max_len = 30  #maximum length of each sentence\n",
    "  batch_size = 6\n",
    "  max_pred = 5\n",
    "  n_layers = 6\n",
    "  n_heads=12\n",
    "  h_dim = 768\n",
    "  d_ff = 3072  #768 * 4\n",
    "  d_k = d_v = 64\n",
    "  n_segments = 2\n",
    "\n",
    "  text1 = ('Hello, how are you? I am Romeo. \\n'\n",
    "      'Hello, Romeo My name is Julliet. Nice to meet you .\\n'\n",
    "      'Nice meet you too.today?\\n'\n",
    "      'Great. My baseball team won the competition.\\n'\n",
    "      'Oh Congratulations, Juliet\\n'\n",
    "      'Thank you Romeo')\n",
    "  \n",
    "  text2=text1.split(\"''\")\n",
    "  text=text2[0]\n",
    "  \n",
    "  sentences = re.sub(\"[.,!?\\\\-]\",'',text.lower()).split('\\n')\n",
    "  word_list = list(set(\" \".join(sentences).split()))\n",
    "  word_dict = {'[PAD]':0, '[CLS]':1, '[SEP]':2, '[MASK]':3}\n",
    "\n",
    "  for i,w in enumerate(word_list):\n",
    "      word_dict[w]=i+4\n",
    "  number_dict = {i:w for i,w in enumerate (word_dict)}\n",
    "  vocab_size = len(word_dict)\n",
    "\n",
    "  token_list = list()\n",
    "  for sentence in sentences:\n",
    "      arr=[word_dict[s] for s in sentence.split()]\n",
    "      token_list.append(arr)\n",
    "\n",
    "  model=BERT()\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "  batch=make_batch()\n",
    "  input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "  #training\n",
    "  for epoch in range(1):\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_clsf = model(input_ids,segment_ids,masked_pos)\n",
    "    # print(logits_lm)\n",
    "    loss_lm=criterion(logits_lm.transpose(1,2),masked_tokens) #for masked LM\n",
    "    print(masked_tokens)\n",
    "    loss_lm=(loss_lm.float()).mean()\n",
    "    loss_clsf=criterion(logits_clsf,isNext)# for sentence classification\n",
    "    loss = loss_lm+loss_clsf\n",
    "    # if (epoch+1)%10==0:\n",
    "      # print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Predict mask tokens ans isNext\n",
    "  model.eval()\n",
    "  epoch_loss1=0\n",
    "  with torch.no_grad():\n",
    "    for i in range(0,5):\n",
    "      input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[4]))\n",
    "      # print(text)\n",
    "      print([number_dict[int(w)] for w in input_ids[0] if number_dict[int(w)] != '[PAD]'])\n",
    "\n",
    "      logits_lm1, logits_clsf1 = model(input_ids,segment_ids,masked_pos)\n",
    "      logits_lm = logits_lm1.data.max(2)[1][0].data.numpy()\n",
    "      print('masked tokens list:',[pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
    "      print('predicted masked tokens list:',[pos for pos in logits_lm if pos !=0])\n",
    "\n",
    "      logits_clsf = logits_clsf1.data.max(1)[1].data.numpy()[0]\n",
    "      print(isNext)\n",
    "      print(logits_lm1.data.max(2))\n",
    "      print(masked_tokens)\n",
    "      print('IsNext: ', True if isNext[0] else False)\n",
    "      print('predict isNext:',True if logits_clsf else False)\n",
    "\n",
    "      loss_lm=criterion(logits_lm1.transpose(1,2),masked_tokens) #for masked LM\n",
    "      loss_lm=(loss_lm.float()).mean()\n",
    "      loss_clsf=criterion(logits_clsf1,isNext)# for sentence classification\n",
    "      loss = loss_lm+loss_clsf\n",
    "      print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "   \n"
   ]
  }
 ]
}