{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using custom dataloader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1MXuCceIBeSCcKzzLGjQhY7ozk3_HnikL",
      "authorship_tag": "ABX9TyP944R2jo4cbG2qzn0cVU4B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nilakshi104/Basic-Transformer-Model/blob/main/Using_custom_dataloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ1LNIMyuDDL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1f35fa-7080-448d-db70-b08f3dde9f82"
      },
      "source": [
        "!pip install torchtext==0.6.0 #run this cell & restart to avoid error while using bleu_score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 23.8MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 30.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 18.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 13.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 51kB 12.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 61kB 12.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 26.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.7.0+cu101)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6.0) (2.23.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6.0) (3.0.4)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed sentencepiece-0.1.95 torchtext-0.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C3ulXOeuS-w"
      },
      "source": [
        "#Pytorch Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "#torchtext package consists of data processing utilities and popular datasets for natural language.\n",
        "import torchtext\n",
        "from torchtext.data import Field, BucketIterator, TabularDataset\n",
        "\n",
        "#Plotting graphs\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "#To tokenize sentence using spacy models\n",
        "import spacy\n",
        "\n",
        "#storing \\n separated data in tabular form using pd.DataFrame\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI64A7hVuY0O"
      },
      "source": [
        "#to reproduce results\n",
        "SEED=1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlx5w-RLgeb9"
      },
      "source": [
        "The code after the ! is not Python but a shell command. It’s an easy way to use these commands in a Jupyter Notebook instead of opening up a terminal and entering the command there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzoCrW3wy6XK"
      },
      "source": [
        "#Downloading dataset\n",
        "# ! mkdir -p '/content/drive/MyDrive/Projects/NLP/data/sequence to sequence /German_to_English Basic TransformerModel/multi30k'\n",
        "# ! wget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/training.tar.gz &&  tar -xf training.tar.gz -C '/content/drive/MyDrive/Projects/NLP/data/sequence to sequence /German_to_English Basic TransformerModel/multi30k' && rm training.tar.gz\n",
        "# ! wget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/validation.tar.gz && tar -xf validation.tar.gz -C '/content/drive/MyDrive/Projects/NLP/data/sequence to sequence /German_to_English Basic TransformerModel/multi30k' && rm validation.tar.gz\n",
        "# ! wget http://www.quest.dcs.shef.ac.uk/wmt16_files_mmt/mmt16_task1_test.tar.gz && tar -xf mmt16_task1_test.tar.gz -C '/content/drive/MyDrive/Projects/NLP/data/sequence to sequence /German_to_English Basic TransformerModel/multi30k' && rm mmt16_task1_test.tar.gz"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbBOwM_syhW3"
      },
      "source": [
        "path='/content/drive/MyDrive/Projects/NLP/data/sequence to sequence /German_to_English Basic TransformerModel/multi30k/'\n",
        "train_en=open(path+'train.en',encoding='utf-8').read().split('\\n')\n",
        "train_de=open(path+'train.de',encoding='utf-8').read().split('\\n')\n",
        "test_en=open(path+'test.en',encoding='utf-8').read().split('\\n')\n",
        "test_de=open(path+'test.de',encoding='utf-8').read().split('\\n')\n",
        "val_en=open(path+'val.en',encoding='utf-8').read().split('\\n')\n",
        "val_de=open(path+'val.de',encoding='utf-8').read().split('\\n')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdOBp763u7lN"
      },
      "source": [
        "#downloading spacy models\n",
        "!python -m spacy download de\n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVzSf2Zbubgg"
      },
      "source": [
        "spacy_de = spacy.load('de')  #loading spacy model for german lang\n",
        "spacy_en = spacy.load('en')  #loading spacy model for english lang"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvwPjvNP-Z-o",
        "outputId": "0b377ab9-51e1-4eb0-dc24-dd752e364337"
      },
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWzHgtPtnikE"
      },
      "source": [
        "## Making Dataset ready for Encoder and Decoder\n",
        "\n",
        "The best way to work with Torchtext is to turn data in spreadsheet format, no matter the original format of data file. As using Torchtext TabularDataset function we can create datasets from spreadsheet formats for that we need to convert data in CSV formate.\n",
        "\n",
        "* step 1)store data in dataframe with two coulmns. First col for source language and second col for target language.\n",
        "\n",
        "* step 2)save dataframe in .csv file\n",
        "\n",
        "* step 3)pass address of train,validation,test .csv files as input to torchtext.data.TabularDataset which will output train_data,validation_data,test_data\n",
        "\n",
        "* step 4)Now index all tokens in german and english using SRC.build_vocab and TRG.build_vocab respectively where SRC and TRG are field class instances \n",
        "\n",
        "* step 5)Creating Batches using BucketIterator. \n",
        "\n",
        "After performing all steps will get train_iterator, valid_iterator, test_iterator with specified batch_size and sentences arranged in ascending order of source length i.e. greman here while creating batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8HiIR8kveGJ"
      },
      "source": [
        "#Define tokenizer\n",
        "def tokenize_de(text):\n",
        "  return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "  return [tok.text for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhAM_WQJtmkL"
      },
      "source": [
        "#defining instance of field class which will perform preprocessing of data. NOTE: Always consider SRC as langauge that is to be translated and TRG as lang to which translation is aimed at. \n",
        "SRC=Field(tokenize=tokenize_de,\n",
        "          init_token='<sos>',\n",
        "          eos_token='<eos>',\n",
        "          lower=True,  #Whether to lowercase the text in field.\n",
        "          batch_first=True)  #since model expects - tensors with batch_dimension first\n",
        "\n",
        "TRG=Field(tokenize=tokenize_en,\n",
        "          init_token='<sos>',\n",
        "          eos_token='<eos>',\n",
        "          lower=True,\n",
        "          batch_first=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WXgm1QItmaN"
      },
      "source": [
        "train_data = {'german':[line for line in train_de],'english':[line for line in train_en]}\n",
        "train_df = pd.DataFrame(train_data, columns=['german','english'])\n",
        "\n",
        "val_data = {'german':[line for line in val_de],'english':[line for line in val_en]}\n",
        "val_df = pd.DataFrame(val_data, columns=['german','english'])\n",
        "\n",
        "test_data = {'german':[line for line in test_de],'english':[line for line in test_en]}\n",
        "test_df = pd.DataFrame(test_data, columns=['german','english'])\n",
        "# remove very long sentences and sentences where translations are \n",
        "# not of roughly equal lengthdf['eng_len'] = df['English'].str.count(' ')\n",
        "# df['fr_len'] = df['French'].str.count(' ')\n",
        "# df = df.query('fr_len < 80 & eng_len < 80')\n",
        "# df = df.query('fr_len < eng_len * 1.5 & fr_len * 1.5 > eng_len')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ewfrw8tEx8VQ"
      },
      "source": [
        "train_df.to_csv(\"/content/drive/MyDrive/Projects/NLP/data/sequence to sequence /German_to_English Basic TransformerModel/multi30k/csv_files/train.csv\", index=False)\n",
        "val_df.to_csv(\"/content/drive/MyDrive/Projects/NLP/data/sequence to sequence /German_to_English Basic TransformerModel/multi30k/csv_files/val.csv\", index=False)\n",
        "test_df.to_csv(\"/content/drive/MyDrive/Projects/NLP/data/sequence to sequence /German_to_English Basic TransformerModel/multi30k/csv_files/test.csv\", index=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4kTNYApyXPD"
      },
      "source": [
        "data_fields= [('german',SRC),('english',TRG)]  #input language followed by output language\n",
        "\n",
        "train_data,valid_data,test_data = torchtext.data.TabularDataset.splits(path='/content/drive/MyDrive/Projects/NLP/data/sequence to sequence /German_to_English Basic TransformerModel/multi30k/csv_files/', train='train.csv', validation='val.csv', test='test.csv',format='csv', fields=data_fields)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIyj8RwXz5gu"
      },
      "source": [
        "SRC.build_vocab(train_data, min_freq=2) #words with occurance less than min_freq are replaced with UNK token\n",
        "TRG.build_vocab(train_data, min_freq=2)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhz9SQ6Yz5X3"
      },
      "source": [
        "# print(TRG.vocab.stoi['the'])\n",
        "# print(TRG.vocab.itos[7])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf00pbmn0z2S"
      },
      "source": [
        "# train_iterator,valid_iterator,test_iterator= BucketIterator.splits(\n",
        "#     (train_data,val_data,test_data),\n",
        "#     batch_size=128, \n",
        "#     sort_key=lambda x: len(x.src), \n",
        "#     # shuffle=True,\n",
        "#     device=device)\n",
        "train_iterator = BucketIterator(train_data, batch_size=128, sort_key=lambda x: len(x.german), shuffle=True,device=device)\n",
        "valid_iterator = BucketIterator(valid_data, batch_size=128, sort_key=lambda x: len(x.german), shuffle=True,device=device)\n",
        "test_iterator = BucketIterator(test_data, batch_size=128, sort_key=lambda x: len(x.german), shuffle=True,device=device)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j6dEq7HtZcr"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqrP1_0I8PfH"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_dim,  #vocabulary size\n",
        "               hid_dim, #second dimension of standard embedding and positional embedding = hid_dim\n",
        "               n_layers,  #no. of layers of encoder =6 (from paper)\n",
        "               n_heads,   # no. of heads in multihead = 8 (from ppr)\n",
        "               pf_dim,  #For positionwise_feedforward layer\n",
        "               dropout,   # 0.1 (from ppr)\n",
        "               device,\n",
        "               max_length=100): #Max 100 words are allowed in sentence\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.device=device\n",
        "    self.tok_embedding=nn.Embedding(input_dim,hid_dim)\n",
        "    self.pos_embedding=nn.Embedding(max_length,hid_dim)\n",
        "    self.layers=nn.ModuleList([EncoderLayer(hid_dim,\n",
        "                                            n_heads,\n",
        "                                            pf_dim,\n",
        "                                            dropout,\n",
        "                                            device)\n",
        "                              for _ in range(n_layers)])\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.scale=torch.sqrt(torch.FloatTensor([hid_dim])).to(device)  #WHY TO(DEVICE)??????\n",
        "\n",
        "  def forward(self,src,src_mask):\n",
        "\n",
        "    #src =[ batch size, src_len]\n",
        "    #src_mask=[batch_size,1,1,src_len]  ...In later steps src_mask is applied on energy of dim=[batch_size,n_heads,query_len,src_len] so aacordingly new dim are added in src_mask \n",
        "\n",
        "    batch_size= src.shape[0]\n",
        "    src_len =src.shape[1]\n",
        "\n",
        "    pos=torch.arange(0,src_len).unsqueeze(0).repeat(batch_size,1).to(self.device)\n",
        "\n",
        "    # pos = tensor([0,1,2,3-----------,src_len-1],\n",
        "    #              [0,1,2,3-----------,src_len-1],\n",
        "    #              [0,1,2,3-----------,src_len-1],\n",
        "    #              [0,1,2,3-----------,src_len-1],\n",
        "    #              [0,1,2,3-----------,src_len-1])\n",
        "\n",
        "    # pos = [batch_size, src_len]\n",
        "\n",
        "    # Before adding std embedding to pos embedding, std embd are multiplied by sqrt(hid_dim) this supposedly reduces variance in the embeddings and model is difficult to train reliably without this scaling factor. \n",
        "    #after adding sqrt(hid_dim)*std_embd to pos_embd, dropout is applied to addition\n",
        "\n",
        "    src = self.dropout((self.tok_embedding(src)*self.scale)+self.pos_embedding(pos))\n",
        "    #src = [batch_size,src_len,hid_dim]\n",
        "\n",
        "    # Different parameters are used in each layer\n",
        "    for layer in self.layers:\n",
        "      src=layer(src,src_mask)\n",
        "    #src=[batch_size,src_len,hid_dim]\n",
        "\n",
        "    return src"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGylkVzq8Pbn"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self,\n",
        "               hid_dim,\n",
        "               n_heads,\n",
        "               pf_dim,\n",
        "               dropout,\n",
        "               device):\n",
        "    \n",
        "    super().__init__() #To reuse base state. Refer this : https://www.youtube.com/watch?v=tX_v8dgb_7I\n",
        "\n",
        "    self.self_attn_layer_norm = nn.LayerNorm(hid_dim) # normalizes the values of the features, i.e. across the hidden dimension, so each feature has a mean of 0 and a standard deviation of 1. This allows neural networks with a larger number of layers, like the Transformer, to be trained easier.\n",
        "    self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "    self.self_attention = MultiHeadAttentionLayer(hid_dim,n_heads,dropout,device)\n",
        "    self.positionwise_feedforward=PositionwiseFeedforwardLayer(hid_dim,pf_dim,dropout)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,src,src_mask):\n",
        "\n",
        "    #src = [batch_size,src_len,hid_dim]\n",
        "    #src_mask = [batch_size,1,1,src_len]   ....It is then unsqueezed so it can be correctly broadcast when applying the mask to the energy\n",
        "\n",
        "    #self_attention\n",
        "    _src,_ = self.self_attention(src,src,src,src_mask)\n",
        "\n",
        "    #dropout is applied on output from self_attention i.e. _src and then src is added followed by layer normalization\n",
        "    src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "    #src =[batch_size, src_len, hid_dim]\n",
        "\n",
        "    #positionwise feedforward\n",
        "    _src = self.positionwise_feedforward(src)\n",
        "\n",
        "    src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "    #src = [batch_size,src_len,hid_dim]\n",
        "\n",
        "    return src"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97zZPPtFtgTz"
      },
      "source": [
        "## MultiHeadAttention & FeedForward Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nfdXon98PXf"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "  def __init__(self,hid_dim,n_heads,dropout,device):\n",
        "\n",
        "    super().__init__()\n",
        "    #print(hid_dim,n_heads,hid_dim % n_heads)\n",
        "    assert hid_dim % n_heads == 0\n",
        "    self.hid_dim = hid_dim\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = hid_dim // n_heads\n",
        "\n",
        "    self.fc_q = nn.Linear(hid_dim,hid_dim)\n",
        "    self.fc_k = nn.Linear(hid_dim,hid_dim)\n",
        "    self.fc_v = nn.Linear(hid_dim,hid_dim)\n",
        "    self.fc_o = nn.Linear(hid_dim,hid_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.scale=torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "  def forward(self,query,key,value,mask=None): #query=key=value=src\n",
        "\n",
        "    #query = [batch_size, query_len, hid_dim]\n",
        "    #query = [batch_size, key_len, hid_dim]\n",
        "    #query = [batch_size, value_len, hid_dim]\n",
        "    #query_len = key_len = value_len\n",
        "\n",
        "    batch_size = query.shape[0]\n",
        "\n",
        "    Q = self.fc_q(query)\n",
        "    K = self.fc_k(key)\n",
        "    V = self.fc_v(value)\n",
        "    #Q = [batch size, query len, hid dim]\n",
        "    #K = [batch size, key len, hid dim]\n",
        "    #V = [batch size, value len, hid dim] \n",
        "\n",
        "    Q = Q.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
        "    K = K.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
        "    V = V.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
        "    #Q = [batch_size, n_heads, query_len, head_dim]\n",
        "    #K = [batch_size, n_heads, key_len, head_dim]\n",
        "    #V = [batch_size, n_heads, value_len, head_dim]\n",
        "\n",
        "    energy = torch.matmul(Q,K.permute(0,1,3,2))/self.scale\n",
        "    #energy = [batch_size, n_heads, query_len,key_len]\n",
        "\n",
        "    if mask is not None:\n",
        "      energy = energy.masked_fill(mask==0,-1e10) #NOTE: mask is boolean matrix though for False,True values in mask 0,1 are used in masked_fill\n",
        "\n",
        "    attention = torch.softmax(energy, dim=-1)\n",
        "    #attention= [batch_size, n_heads, query_len,key_len]\n",
        "\n",
        "    x = torch.matmul(self.dropout(attention),V)\n",
        "    #x= [batch_size, n_heads, query_len,head_dim]\n",
        "\n",
        "    x=x.permute(0,2,1,3).contiguous()\n",
        "    #x=[batch_size,query_len,n_heads,head_dim]\n",
        "\n",
        "    #before applying view,contiguous is applied\n",
        "    x=x.view(batch_size,-1,self.hid_dim)\n",
        "    #x = [batch size, query len, hid dim]\n",
        "\n",
        "    x=self.fc_o(x)\n",
        "    #x = [batch size, query len, hid dim]\n",
        "\n",
        "    return x,attention"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogj7u2t28PRk"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "  def __init__(self,hid_dim,pf_dim,dropout):\n",
        "    super().__init__()\n",
        "    self.fc_1 = nn.Linear(hid_dim,pf_dim)\n",
        "    self.fc_2 = nn.Linear(pf_dim,hid_dim)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    #x = [batch size, seq len, hid dim]\n",
        "        \n",
        "    x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "    #x = [batch size, seq len, pf_dim]\n",
        "\n",
        "    x=self.fc_2(x)\n",
        "    #x = [batch size, seq len, hid dim]\n",
        "\n",
        "    return x"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ojkXSBZt3k9"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stTtG_Dh8PO1"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,\n",
        "               output_dim,\n",
        "               hid_dim,\n",
        "               n_layers,\n",
        "               n_heads,\n",
        "               pf_dim,\n",
        "               dropout,\n",
        "               device,\n",
        "               max_length=100): #here also for max 100 words pos embeddings are taken\n",
        "    super().__init__()\n",
        "    self.device=device\n",
        "    self.tok_embedding=nn.Embedding(output_dim,hid_dim)\n",
        "    self.pos_embedding = nn.Embedding(max_length,hid_dim)\n",
        "    self.layers = nn.ModuleList([DecoderLayer(hid_dim,\n",
        "                                              n_heads,\n",
        "                                              pf_dim,\n",
        "                                              dropout,\n",
        "                                              device)\n",
        "                                for _ in range(n_layers)])\n",
        "    \n",
        "    self.fc_out = nn.Linear(hid_dim,output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "  def forward(self, trg, enc_src, trg_mask, src_mask):\n",
        "\n",
        "    #trg = [batch_size,trg_len]\n",
        "    #enc_src = [batch_size,src_len,hid_dim]\n",
        "    #trg_mask = [batch_size,1,trg_len,trg_len]\n",
        "    #src_mask = [batch_size,1,1,src_len]\n",
        "\n",
        "    batch_size=trg.shape[0]\n",
        "    trg_len=trg.shape[1]\n",
        "\n",
        "    pos=torch.arange(0,trg_len).unsqueeze(0).repeat(batch_size,1).to(self.device)\n",
        "    #pos =[batch_size,trg_len]\n",
        "\n",
        "    trg = self.dropout((self.tok_embedding(trg)*self.scale)+self.pos_embedding(pos))\n",
        "    #trg = [batch size, trg len, hid dim]\n",
        "\n",
        "    for layer in self.layers:\n",
        "      trg,attention=layer(trg,enc_src,trg_mask,src_mask)\n",
        "      #trg=[batch_size,trg_len,hid_dim]\n",
        "      #attention=[batch_size,n_heads,trg_len,src_len]\n",
        "\n",
        "    output=self.fc_out(trg)\n",
        "\n",
        "    return output,attention"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0khoJ-t8PL1"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self,\n",
        "              hid_dim,\n",
        "              n_heads,\n",
        "              pf_dim,\n",
        "              dropout,\n",
        "              device):\n",
        "    super().__init__()\n",
        "    self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "    self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
        "    self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "    self.self_attention = MultiHeadAttentionLayer(hid_dim,n_heads,dropout,device)\n",
        "    self.encoder_attention = MultiHeadAttentionLayer(hid_dim,n_heads,dropout,device)\n",
        "    self.positionwise_feedforward =PositionwiseFeedforwardLayer(hid_dim,pf_dim,dropout)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,trg, enc_src, trg_mask,src_mask):\n",
        "\n",
        "    #trg=[batch size, trg len, hid dim]\n",
        "    #enc_src=[batch_size,src_len,hid_dim]\n",
        "    #trg_mask = [batch_size,1,trg_len,trg_len]\n",
        "    #src_mask = [batch_size,1,1,src_len]\n",
        "\n",
        "    #self_attention\n",
        "    _trg,_ = self.self_attention(trg,trg,trg,trg_mask)\n",
        "\n",
        "    trg = self.self_attn_layer_norm(trg+self.dropout(_trg))\n",
        "    #trg =[batch_size,trg_len,hid_dim]\n",
        "\n",
        "    #encoder_attention\n",
        "    _trg,attention = self.encoder_attention(trg,enc_src,enc_src,src_mask) #attention op from last MultiHeadAttention of decoder\n",
        "\n",
        "    trg=self.enc_attn_layer_norm(trg+self.dropout(_trg))\n",
        "    #trg = [batch_size,trg_len,hid_dim]\n",
        "\n",
        "    #positionwise feedforward\n",
        "\n",
        "    _trg = self.positionwise_feedforward(trg)\n",
        "\n",
        "    trg=self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "    #trg = [batch size, trg len, hid dim]\n",
        "    #attention = [batch size, n heads, trg len, src len]\n",
        "    return trg,attention #attention is returned as we can plot it later to observe how may attention is paid by ip to op sentences\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrhwnlaZt-Fl"
      },
      "source": [
        "## Building Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvJNjZ8Y9dV5"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self,\n",
        "               encoder,\n",
        "               decoder,\n",
        "               src_pad_idx,\n",
        "               trg_pad_idx,\n",
        "               device):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder=encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.trg_pad_idx = trg_pad_idx\n",
        "    self.device = device\n",
        "\n",
        "  def make_src_mask(self,src):\n",
        "    #src=[batch_size,src_len]\n",
        "    src_mask=(src!=self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    #src_mask=[batch_size,1,1,src_len]\n",
        "\n",
        "    return src_mask\n",
        "\n",
        "  def make_trg_mask(self,trg):\n",
        "    #trg=[batch_size,trg_len]\n",
        "\n",
        "    trg_pad_mask=(trg!=self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "    #trg_pad_mask=[batch_size,1,1,trg_len]\n",
        "\n",
        "    trg_len=trg.shape[1]\n",
        "    trg_sub_mask=torch.tril(torch.ones((trg_len,trg_len),device=self.device)).bool()\n",
        "\n",
        "    #trg_sub_mask=[trg_len,trg_len]\n",
        "\n",
        "    trg_mask= trg_pad_mask & trg_sub_mask\n",
        "    #trg_mask =[batch_size,1,trg_len,trg_len]\n",
        "\n",
        "    return trg_mask\n",
        "\n",
        "  def forward(self,src,trg):\n",
        "\n",
        "    #src=[batch_size,src_len]\n",
        "    #trg=[batch_size,trg_len]\n",
        "\n",
        "    src_mask=self.make_src_mask(src)\n",
        "    trg_mask=self.make_trg_mask(trg)\n",
        "\n",
        "    #src_mask = [batch size, 1, 1, src len]\n",
        "    #trg_mask = [batch size, 1, trg len, trg len]\n",
        "\n",
        "    enc_src=self.encoder(src,src_mask)\n",
        "    #enc_src=[batch_size,src_len,hid_dim]\n",
        "\n",
        "    output,attention=self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "    #output=[batch_size,trg_len,output_dim]\n",
        "    #attention=[batch_size,n_heads,trg_len,src_len]\n",
        "\n",
        "    return output, attention"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_tC4wmu9dTJ"
      },
      "source": [
        "INPUT_DIM=len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "HID_DIM=256\n",
        "ENC_LAYERS=3\n",
        "DEC_LAYERS=3\n",
        "ENC_HEADS=8\n",
        "DEC_HEADS=8\n",
        "ENC_PF_DIM=512\n",
        "DEC_PF_DIM=512\n",
        "ENC_DROPOUT=0.1\n",
        "DEC_DROPOUT=0.1\n",
        "\n",
        "enc=Encoder(INPUT_DIM,\n",
        "            HID_DIM,\n",
        "            ENC_LAYERS,\n",
        "            ENC_HEADS,\n",
        "            ENC_PF_DIM,\n",
        "            ENC_DROPOUT,\n",
        "            device)\n",
        "\n",
        "dec=Decoder(OUTPUT_DIM,\n",
        "            HID_DIM,\n",
        "            DEC_LAYERS,\n",
        "            DEC_HEADS,\n",
        "            DEC_PF_DIM,\n",
        "            DEC_DROPOUT,\n",
        "            device)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhGM1elq9dQh"
      },
      "source": [
        "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-XPF7SH9j4j",
        "outputId": "8eb876c4-11ba-4953-9101-4a17f1b03375"
      },
      "source": [
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 9,039,366 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIIVKsCG-uac"
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:  # hasaatr: to check if an object has the given named attribute and return true if present, else false.\n",
        "        nn.init.xavier_uniform_(m.weight.data)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km3g0381-v73",
        "outputId": "fa137635-8a8d-47c7-ef4a-315c559944f4"
      },
      "source": [
        "model.apply(initialize_weights)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embedding): Embedding(7855, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embedding): Embedding(5894, 256)\n",
              "    (pos_embedding): Embedding(100, 256)\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (encoder_attention): MultiHeadAttentionLayer(\n",
              "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
              "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (fc_out): Linear(in_features=256, out_features=5894, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HhL5tgJ-v5D"
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_Wipfra-v2p"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #as while caculating loss only trg_pad_idx is considered"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JXC3taF-vzx"
      },
      "source": [
        "def train(model,iterator,optimizer,criterion,clip):\n",
        "  model.train()\n",
        "  epoch_loss=0\n",
        "\n",
        "  for i,batch in enumerate(iterator):\n",
        "    # print(batch)\n",
        "    src=batch.german\n",
        "    trg=batch.english\n",
        "    optimizer.zero_grad()\n",
        "    output,_=model(src,trg[:,:-1])\n",
        "    #output=[batch_size,trg_len-1,output_dim]\n",
        "    #trg=[batch_size,trg_len]\n",
        "    output_dim=output.shape[-1]\n",
        "    output=output.contiguous().view(-1,output_dim)  #since nn.crossentropyloss expects output size=(N,C) and label size=(C) \n",
        "    trg=trg[:,1:].contiguous().view(-1)\n",
        "    #output=[batch size * trg len - 1, output dim]\n",
        "    #trg=[batch size * trg len - 1]\n",
        "    loss=criterion(output,trg)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm(model.parameters(),clip)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "    \n",
        "  return epoch_loss/len(iterator)\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWnHnWia-vxQ"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for i,batch in enumerate (iterator):\n",
        "      src=batch.german\n",
        "      trg=batch.english\n",
        "      output, _ = model(src,trg[:,:-1])\n",
        "      #output = [batch_size,trg_len-1,output dim]\n",
        "      #trg = [batch_size, trg_len]\n",
        "\n",
        "      output_dim =output.shape[-1]\n",
        "      output=output.contiguous().view(-1,output_dim)\n",
        "      trg =trg[:,1:].contiguous().view(-1)\n",
        "      #output = [batch_size*trg len - 1, output dim]\n",
        "      #trg = [batch_size*trg_len-1]\n",
        "      loss=criterion(output,trg)\n",
        "      epoch_loss+=loss.item()\n",
        "  return epoch_loss/len(iterator)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6liFDjS-vup"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzxHe2ZS-vrc",
        "outputId": "dd3f44f7-7b5a-42ea-fec7-791dc9996413"
      },
      "source": [
        "N_EPOCHS=10\n",
        "CLIP=1\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "  start_time = time.time()\n",
        "  train_loss = train(model,train_iterator,optimizer,criterion,CLIP)\n",
        "  valid_loss = evaluate(model,valid_iterator,criterion)\n",
        "\n",
        "  end_time = time.time()\n",
        "  epoch_mins,epoch_secs=epoch_time(start_time,end_time)\n",
        "\n",
        "  if valid_loss < best_valid_loss:\n",
        "    best_valid_loss = valid_loss\n",
        "    torch.save(model.state_dict(),'/content/drive/MyDrive/Projects/NLP/data/sequence to sequence /German_to_English Basic TransformerModel/multi30k without using custom dataloader/checkpoints/model.pt')\n",
        "\n",
        "  print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "  print(f'\\tTrainLoss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 20s\n",
            "\tTrainLoss: 4.232 | Train PPL:  68.868\n",
            "\t Val. Loss: 3.068 |  Val. PPL:  21.494\n",
            "Epoch: 02 | Time: 0m 20s\n",
            "\tTrainLoss: 2.808 | Train PPL:  16.570\n",
            "\t Val. Loss: 2.358 |  Val. PPL:  10.565\n",
            "Epoch: 03 | Time: 0m 21s\n",
            "\tTrainLoss: 2.231 | Train PPL:   9.305\n",
            "\t Val. Loss: 2.023 |  Val. PPL:   7.563\n",
            "Epoch: 04 | Time: 0m 21s\n",
            "\tTrainLoss: 1.879 | Train PPL:   6.550\n",
            "\t Val. Loss: 1.862 |  Val. PPL:   6.434\n",
            "Epoch: 05 | Time: 0m 20s\n",
            "\tTrainLoss: 1.634 | Train PPL:   5.123\n",
            "\t Val. Loss: 1.776 |  Val. PPL:   5.907\n",
            "Epoch: 06 | Time: 0m 20s\n",
            "\tTrainLoss: 1.446 | Train PPL:   4.246\n",
            "\t Val. Loss: 1.692 |  Val. PPL:   5.429\n",
            "Epoch: 07 | Time: 0m 20s\n",
            "\tTrainLoss: 1.294 | Train PPL:   3.646\n",
            "\t Val. Loss: 1.678 |  Val. PPL:   5.355\n",
            "Epoch: 08 | Time: 0m 20s\n",
            "\tTrainLoss: 1.169 | Train PPL:   3.220\n",
            "\t Val. Loss: 1.668 |  Val. PPL:   5.304\n",
            "Epoch: 09 | Time: 0m 20s\n",
            "\tTrainLoss: 1.058 | Train PPL:   2.880\n",
            "\t Val. Loss: 1.676 |  Val. PPL:   5.344\n",
            "Epoch: 10 | Time: 0m 20s\n",
            "\tTrainLoss: 0.961 | Train PPL:   2.614\n",
            "\t Val. Loss: 1.702 |  Val. PPL:   5.483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEFbYTHv-vl_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a5c7423-8fa2-4d7d-de27-c4777f54a0fb"
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Projects/NLP/data/sequence to sequence /German_to_English Basic TransformerModel/multi30k without using custom dataloader/checkpoints/model.pt'))\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 1.711 | Test PPL:   5.534 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QdkyyuQ-vjr"
      },
      "source": [
        "def translate_sentence(sentence,src_field,trg_field,model,device,max_length=50):\n",
        "  model.eval()\n",
        "  if isinstance (sentence,str):\n",
        "    tokenize_de=spacy.load('de')\n",
        "    tokens=[token.text for token in tokenize_de(setence)]\n",
        "  else:\n",
        "    tokens=[token.lower() for token in sentence]\n",
        "\n",
        "  #adding sos and eos token to tokens\n",
        "  tokens=[src_field.init_token]+tokens+[src_field.eos_token]\n",
        "\n",
        "  #converting tokens(strings) to indexes \n",
        "  src_indexes=[src_field.vocab.stoi[token] for token in tokens]\n",
        "  #Convert to tensor and unsqueeze first dim so batch size=1 \n",
        "  src_tensor=torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "  src_mask=model.make_src_mask(src_tensor)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    enc_src=model.encoder(src_tensor,src_mask)\n",
        "\n",
        "  trg_indexes=[trg_field.vocab.stoi[trg_field.init_token]]\n",
        "\n",
        "  for i in range(max_length):\n",
        "    #convert trg_indexes to tensor\n",
        "    trg_tensor=torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "    trg_mask=model.make_trg_mask(trg_tensor)\n",
        "    with torch.no_grad():\n",
        "      # so bascically what happens in decoder part in evaluation, no pad token is fed as input so trg_pad_mask =True always only trg_sub_mask changes with each word in sentence (for evaluation we considered one batch only)\n",
        "      output,attention=model.decoder(trg_tensor,enc_src,trg_mask,src_mask)\n",
        "      pred_token=output.argmax(2)[:,-1].item()\n",
        "      trg_indexes.append(pred_token)\n",
        "\n",
        "      if pred_token==trg_field.vocab.stoi[trg_field.eos_token]:\n",
        "        break\n",
        "\n",
        "  trg_tokens=[trg_field.vocab.itos[i] for i in trg_indexes]\n",
        "\n",
        "  return trg_tokens[1:],attention"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM0ANdcsPFew"
      },
      "source": [
        "## Plotting Attention:\n",
        "\n",
        "On X axes:- source sentence (german language) including sos +sentence+ eos\n",
        "\n",
        "On Y axes:- target sentence (english language) output of deocder i.e. sentence + eos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1HVfGji-vg_"
      },
      "source": [
        "def display_attention(sentence,translation,attention,n_heads=8,n_rows=4,n_cols=2):\n",
        "  assert n_rows*n_cols == n_heads\n",
        "  fig = plt.figure(figsize=(15,25))\n",
        "  for i in range(n_heads):\n",
        "    ax=fig.add_subplot(n_rows,n_cols,i+1)\n",
        "    _attention=attention.squeeze(0)[i].cpu().detach().numpy() #1. transfer attention to cpu from gpu, 2.detach() is used as grad=False, 3.Attention tensor is converted to numpy for plotting\n",
        "    cax=ax.matshow(_attention,cmap='bone')\n",
        "    ax.tick_params(labelsize=12)\n",
        "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'],rotation=45)\n",
        "    ax.set_yticklabels(['']+translation) \n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plp3NwMs-veW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "e6f54136-893a-4faa-889b-a77ce9a6d7b9"
      },
      "source": [
        "example_idx = 17\n",
        "\n",
        "src=vars(train_data.examples[example_idx])['german']\n",
        "trg=vars(train_data.examples[example_idx])['english']\n",
        "\n",
        "print(f'src = {src}')\n",
        "print(f'trg = {trg}')\n",
        "\n",
        "translation, attention = translate_sentence(src, SRC, TRG, model, device)\n",
        "\n",
        "print(f'predicted trg ={translation}')\n",
        "\n",
        "display_attention(src, translation, attention)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src = ['ein', 'kleines', 'mädchen', 'sitzt', 'vor', 'einem', 'großen', 'gemalten', 'regenbogen', '.']\n",
            "trg = ['a', 'little', 'girl', 'is', 'sitting', 'in', 'front', 'of', 'a', 'large', 'painted', 'rainbow', '.']\n",
            "predicted trg =['a', 'little', 'girl', 'is', 'sitting', 'in', 'front', 'of', 'a', 'large', 'rainbow', '.', '<eos>']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x1800 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seKoBxXQ-vbf"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def calculate_bleu(data,src_field,trg_field,model,device,max_len=50):\n",
        "  trgs=[]\n",
        "  pred_trgs=[]\n",
        "\n",
        "  for datum in data:\n",
        "    src=vars(datum)['german']\n",
        "    trg=vars(datum)['english']\n",
        "\n",
        "    pred_trg,_ = translate_sentence(src,src_field,trg_field,model,device,max_len)\n",
        "\n",
        "    #cutoff <eos> token\n",
        "    pred_trg=pred_trg[:-1]\n",
        "    pred_trgs.append(pred_trg)\n",
        "    trgs.append([trg])\n",
        "\n",
        "  return bleu_score(pred_trgs, trgs)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1MWwwV_-uTZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a02e5736-897a-4f37-e35b-90b353a02696"
      },
      "source": [
        "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
        "\n",
        "print(f'BLEU score = {bleu_score*100:.2f}')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU score = 35.60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbl2B8UR7-tq"
      },
      "source": [
        "## Yet to complete (Rough Work)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYf5pfLdwCgt"
      },
      "source": [
        "#prepare data\n",
        "\n",
        "SOS_token=0\n",
        "EOS_token=1\n",
        "class Lang():\n",
        "  def __init__(self,name):\n",
        "    self.name=name\n",
        "    self.word2index={'SOS':0,'EOS':1,'UNK':2,'PAD':3}\n",
        "    self.index2word={0:'SOS',1:'EOS',2:'UNK',3:'PAD'}\n",
        "    self.word2count={}\n",
        "    self.keepwords=[]\n",
        "    self.index=4\n",
        "\n",
        "  def addWord(self,text):\n",
        "    words=[tok.text for tok in spacy_fr.tokenizer(text.lower())] if (self.name=='french') else [tok.text for tok in spacy_en.tokenizer(text.lower())]\n",
        "    for word in words:\n",
        "      # print(word)\n",
        "      if word not in self.word2index:\n",
        "        self.word2index[word]=self.index\n",
        "        self.index2word[self.index]=word\n",
        "        self.word2count[word]=1\n",
        "        self.index+=1\n",
        "      else:\n",
        "        self.word2count[word]+=1\n",
        "\n",
        "  def trimvocab(self,min_count):\n",
        "    for word,count in self.word2count.items():\n",
        "      if count>=min_count:\n",
        "        self.keepwords.append(word)\n",
        "      \n",
        "    print('Initial no of words={0} and after trimming no of words={1}'.format(len(self.word2count),len(self.keepwords)))\n",
        "    self.word2index={'SOS':0,'EOS':1,'UNK':2,'PAD':3}\n",
        "    self.index2word={0:'SOS',1:'EOS',2:'UNK',3:'PAD'}\n",
        "    self.index=4\n",
        "\n",
        "    for word in self.keepwords:\n",
        "      self.addWord(word)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MM6z89RI2-6b",
        "outputId": "7ca68b4d-7678-4db1-e015-5e278b628467"
      },
      "source": [
        "english=Lang('english')\n",
        "\n",
        "book=['Girls are there','50m Girls am glade to meet YOU','GiRls ARE glade to MeeT yOU ThEre']\n",
        "for sent in book:\n",
        "  english.addWord(sent)\n",
        "\n",
        "print(english.word2index)\n",
        "print(english.index2word)\n",
        "print(english.word2count)\n",
        "\n",
        "english.trimvocab(2)\n",
        "print(english.word2index)\n",
        "print(english.index2word)\n",
        "print(english.word2count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'SOS': 0, 'EOS': 1, 'UNK': 2, 'PAD': 3, 'girls': 4, 'are': 5, 'there': 6, '50': 7, 'm': 8, 'am': 9, 'glade': 10, 'to': 11, 'meet': 12, 'you': 13}\n",
            "{0: 'SOS', 1: 'EOS', 2: 'UNK', 3: 'PAD', 4: 'girls', 5: 'are', 6: 'there', 7: '50', 8: 'm', 9: 'am', 10: 'glade', 11: 'to', 12: 'meet', 13: 'you'}\n",
            "{'girls': 3, 'are': 2, 'there': 2, '50': 1, 'm': 1, 'am': 1, 'glade': 2, 'to': 2, 'meet': 2, 'you': 2}\n",
            "Initial no of words=10 and after trimming no of words=7\n",
            "{'SOS': 0, 'EOS': 1, 'UNK': 2, 'PAD': 3, 'girls': 4, 'are': 5, 'there': 6, 'glade': 7, 'to': 8, 'meet': 9, 'you': 10}\n",
            "{0: 'SOS', 1: 'EOS', 2: 'UNK', 3: 'PAD', 4: 'girls', 5: 'are', 6: 'there', 7: 'glade', 8: 'to', 9: 'meet', 10: 'you'}\n",
            "{'girls': 1, 'are': 1, 'there': 1, '50': 1, 'm': 1, 'am': 1, 'glade': 1, 'to': 1, 'meet': 1, 'you': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60wqpVpgDDJz",
        "outputId": "0abdb4c5-fce5-4674-8e76-71ed754901f4"
      },
      "source": [
        "new_book=[]\n",
        "for sent in book:\n",
        "  new_word=''\n",
        "  for word in spacy_en(sent.lower()):\n",
        "    # print(word.text)\n",
        "    if word.text in english.word2index.keys():\n",
        "      new_word=new_word+word.text+' '\n",
        "    else:\n",
        "      new_word=new_word+'UNK'+' '\n",
        "  new_book.append(new_word.strip())\n",
        "\n",
        "new_book"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['girls are there',\n",
              " 'UNK UNK girls UNK glade to meet you',\n",
              " 'girls are glade to meet you there']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    }
  ]
}